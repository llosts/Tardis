{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c0d8c1",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook builds predictive models to forecast train delays based on the cleaned SNCF dataset. We'll compare several algorithms and select the best performing model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8ef289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e44c0f8",
   "metadata": {},
   "source": [
    "## Loading and Preparing Data\n",
    "First, let's load the cleaned dataset that we prepared in the EDA notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ffcdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned dataset\n",
    "print(\"Loading cleaned dataset...\")\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982d4148",
   "metadata": {},
   "source": [
    "## Feature Selection and Target Definition\n",
    "Let's define our features and target variable for the predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cb12f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features and target\n",
    "print(\"\\nPreparing features and target variables...\")\n",
    "\n",
    "# Target variable: Average delay of all trains at arrival\n",
    "target = \"Average delay of all trains at arrival\"\n",
    "\n",
    "# Features to use for prediction\n",
    "numerical_features = [\n",
    "    \"Average journey time\",\n",
    "    \"Number of scheduled trains\",\n",
    "    \"Number of cancelled trains\",\n",
    "    \"Number of trains delayed at departure\",\n",
    "    \"Average delay of late trains at departure\",\n",
    "    \"Average delay of all trains at departure\",\n",
    "    \"Pct delay due to external causes\",\n",
    "    \"Pct delay due to infrastructure\",\n",
    "    \"Pct delay due to traffic management\",\n",
    "    \"Pct delay due to rolling stock\",\n",
    "    \"Pct delay due to station management and equipment reuse\",\n",
    "    \"Pct delay due to passenger handling (crowding, disabled persons, connections)\",\n",
    "    \"Year\",\n",
    "    \"Month\",\n",
    "]\n",
    "\n",
    "categorical_features = [\"Service\", \"Departure station\", \"Arrival station\", \"Season\"]\n",
    "\n",
    "# Filter features that actually exist in the dataframe\n",
    "numerical_features = [f for f in numerical_features if f in df.columns]\n",
    "categorical_features = [f for f in categorical_features if f in df.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f768194",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "Check for and handle any remaining missing values in our selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7db5656",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any missing values in our features and target\n",
    "print(\"\\nChecking missing values in selected features:\")\n",
    "features_df = df[\n",
    "    numerical_features + categorical_features + [target]\n",
    "].copy()  # Create a copy\n",
    "print(features_df.isna().sum())\n",
    "\n",
    "# Fill missing values\n",
    "# For numerical features\n",
    "for col in numerical_features:\n",
    "    features_df.loc[:, col] = features_df[col].fillna(features_df[col].median())\n",
    "\n",
    "# For categorical features\n",
    "for col in categorical_features:\n",
    "    features_df.loc[:, col] = features_df[col].fillna(features_df[col].mode()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c440e4",
   "metadata": {},
   "source": [
    "## Train-Test Split\n",
    "Let's split our data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5408efac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into features and target\n",
    "X = features_df.drop(target, axis=1)\n",
    "y = features_df[target]\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.5, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape}\")\n",
    "print(f\"Test set size: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5318cd6",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "Set up preprocessing pipelines for numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0619c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing for numeric and categorical data\n",
    "numeric_transformer = Pipeline(\n",
    "    steps=[(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "categorical_transformer = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numerical_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d038c606",
   "metadata": {},
   "source": [
    "## Model Definition and Evaluation Function\n",
    "Let's define the models we'll compare and create a function to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and compare multiple models\n",
    "models = {\n",
    "    \"Linear Regression\": LinearRegression(),\n",
    "    \"Decision Tree\": DecisionTreeRegressor(random_state=42),\n",
    "    \"Random Forest\": RandomForestRegressor(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(random_state=42),\n",
    "}\n",
    "\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_train, X_test, y_train, y_test, preprocessor):\n",
    "    # Create pipeline with preprocessing and model\n",
    "    pipeline = Pipeline(steps=[(\"preprocessor\", preprocessor), (\"model\", model)])\n",
    "\n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"model\": pipeline,\n",
    "        \"rmse\": rmse,\n",
    "        \"mae\": mae,\n",
    "        \"r2\": r2,\n",
    "        \"predictions\": y_pred,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662e9619",
   "metadata": {},
   "source": [
    "## Model Training and Comparison\n",
    "Now let's train all our models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151ff487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate all models\n",
    "print(\"\\nTraining and evaluating models:\")\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    print(f\"Training {name}...\")\n",
    "    results[name] = evaluate_model(\n",
    "        model, X_train, X_test, y_train, y_test, preprocessor\n",
    "    )\n",
    "    print(\n",
    "        f\"{name} - RMSE: {results[name]['rmse']:.2f}, MAE: {results[name]['mae']:.2f}, R²: {results[name]['r2']:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d417b",
   "metadata": {},
   "source": [
    "## Visualizing Model Comparison\n",
    "Let's create a visual comparison of our models' performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b89d2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "plt.figure(figsize=(12, 6))\n",
    "model_names = list(results.keys())\n",
    "rmse_values = [results[name][\"rmse\"] for name in model_names]\n",
    "r2_values = [results[name][\"r2\"] for name in model_names]\n",
    "\n",
    "x = range(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar([i - width / 2 for i in x], rmse_values, width, label=\"RMSE\")\n",
    "plt.bar([i + width / 2 for i in x], r2_values, width, label=\"R²\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Model Comparison\")\n",
    "plt.xticks(x, model_names)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_comparison.png\")\n",
    "plt.close()\n",
    "\n",
    "# Select the best model based on RMSE\n",
    "best_model_name = min(results, key=lambda k: results[k][\"rmse\"])\n",
    "best_model = results[best_model_name][\"model\"]\n",
    "print(\n",
    "    f\"\\nBest model: {best_model_name} with RMSE: {results[best_model_name]['rmse']:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c37b9a",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Let's fine-tune our best model to improve its performance further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e92b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the best model using Grid Search\n",
    "print(\"\\nFine-tuning the best model...\")\n",
    "\n",
    "if best_model_name == \"Random Forest\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [50, 100, 200],\n",
    "        \"model__max_depth\": [None, 10, 20, 30],\n",
    "        \"model__min_samples_split\": [2, 5, 10],\n",
    "    }\n",
    "elif best_model_name == \"Gradient Boosting\":\n",
    "    param_grid = {\n",
    "        \"model__n_estimators\": [50, 100, 200],\n",
    "        \"model__learning_rate\": [0.01, 0.1, 0.2],\n",
    "        \"model__max_depth\": [3, 5, 7],\n",
    "    }\n",
    "elif best_model_name == \"Decision Tree\":\n",
    "    param_grid = {\n",
    "        \"model__max_depth\": [None, 10, 20, 30],\n",
    "        \"model__min_samples_split\": [2, 5, 10],\n",
    "        \"model__min_samples_leaf\": [1, 2, 4],\n",
    "    }\n",
    "else:  # Linear Regression\n",
    "    # LinearRegression doesn't have many hyperparameters to tune\n",
    "    param_grid = {}\n",
    "\n",
    "if param_grid:\n",
    "    grid_search = GridSearchCV(\n",
    "        best_model, param_grid, cv=5, scoring=\"neg_root_mean_squared_error\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {-grid_search.best_score_:.2f}\")\n",
    "\n",
    "    # Update best model\n",
    "    best_model = grid_search.best_estimator_\n",
    "\n",
    "    # Evaluate tuned model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Tuned model - RMSE: {rmse:.2f}, R²: {r2:.2f}\")\n",
    "else:\n",
    "    print(\"Skipping hyperparameter tuning for Linear Regression.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b96bd8c",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "For tree-based models, let's analyze feature importance to understand what factors most influence train delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b509ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for the best model (if applicable)\n",
    "if best_model_name in [\"Random Forest\", \"Gradient Boosting\", \"Decision Tree\"]:\n",
    "    # Get feature names after preprocessing\n",
    "    feature_names = []\n",
    "\n",
    "    # Get numeric feature names (these stay the same)\n",
    "    feature_names.extend(numerical_features)\n",
    "\n",
    "    # Get one-hot encoded feature names\n",
    "    if categorical_features:\n",
    "        # Fit the preprocessor to get the transformed feature names\n",
    "        preprocessor.fit(X)\n",
    "\n",
    "        # Get the one-hot encoder\n",
    "        ohe = preprocessor.named_transformers_[\"cat\"].named_steps[\"onehot\"]\n",
    "\n",
    "        # Get all one-hot encoded feature names in one go\n",
    "        categorical_feature_names = ohe.get_feature_names_out(\n",
    "            categorical_features\n",
    "        ).tolist()\n",
    "\n",
    "        feature_names.extend(categorical_feature_names)\n",
    "\n",
    "    # Extract feature importances\n",
    "    importances = best_model.named_steps[\"model\"].feature_importances_\n",
    "\n",
    "    # Sort feature importances in descending order\n",
    "    indices = np.argsort(importances)[::-1]\n",
    "\n",
    "    # Plot the feature importances of the top 15 features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(f\"Top 15 Feature Importances for {best_model_name}\")\n",
    "\n",
    "    # Get top 15 features\n",
    "    n_features = min(15, len(feature_names))\n",
    "    top_indices = indices[:n_features]\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.barh(range(n_features), importances[top_indices], align=\"center\")\n",
    "    plt.yticks(np.arange(n_features), [feature_names[i] for i in top_indices])\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613e0b0e",
   "metadata": {},
   "source": [
    "## Model Performance Visualization\n",
    "Let's visualize how well our model's predictions match the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc3bf9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs. predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, best_model.predict(X_test), alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \"k--\", lw=2)\n",
    "plt.xlabel(\"Actual Delay\")\n",
    "plt.ylabel(\"Predicted Delay\")\n",
    "plt.title(f\"Actual vs. Predicted Delays ({best_model_name})\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"actual_vs_predicted.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac2b050",
   "metadata": {},
   "source": [
    "## Save Model for Dashboard\n",
    "Finally, let's save our model and relevant information for use in the Streamlit dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aafa29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "print(\"\\nSaving the best model...\")\n",
    "with open(\"tardis_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(best_model, f)\n",
    "\n",
    "# Save the feature lists for later use\n",
    "model_info = {\n",
    "    \"model_name\": best_model_name,\n",
    "    \"numerical_features\": numerical_features,\n",
    "    \"categorical_features\": categorical_features,\n",
    "    \"target\": target,\n",
    "    \"metrics\": {\"rmse\": rmse, \"r2\": r2},\n",
    "}\n",
    "\n",
    "with open(\"model_info.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model_info, f)\n",
    "\n",
    "print(f\"Best model ({best_model_name}) saved to 'tardis_model.pkl'\")\n",
    "print(\"Model information saved to 'model_info.pkl'\")\n",
    "print(\"\\nModel training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
