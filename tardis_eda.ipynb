{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f19bf32",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook performs exploratory data analysis on SNCF train delay data. We'll clean the data, explore patterns, and prepare it for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a929936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "# Set visualization style - updated for compatibility with newer versions\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")  # Updated style name\n",
    "sns.set_theme()  # Use the default seaborn theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300db0da",
   "metadata": {},
   "source": [
    "## Data Loading and Initial Exploration\n",
    "Let's load the dataset and take a first look at its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f885cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\nDataset Overview:\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check column info\n",
    "print(\"\\nColumn information:\")\n",
    "df.info()\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "missing_values = df.isna().sum()\n",
    "missing_pct = (df.isna().sum() / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\"Missing Values\": missing_values, \"Percentage\": missing_pct})\n",
    "print(missing_df[missing_df[\"Missing Values\"] > 0])\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241b317",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing\n",
    "Now we'll clean the data by handling missing values, converting data types, and creating additional features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da5dce4",
   "metadata": {},
   "source": [
    "## Normalizing String Fields\n",
    "To ensure consistency and improve data quality, we normalize the names of various string fields such as \"Service,\" \"Departure station,\" and \"Arrival station.\" This process involves:\n",
    "\n",
    "1. **Trimming Whitespace**: Removing any leading or trailing spaces.\n",
    "2. **Standardizing Case**: Converting text to title case for uniformity.\n",
    "3. **Removing Extra Spaces**: Replacing multiple spaces with a single space.\n",
    "\n",
    "This normalization step helps in avoiding mismatches caused by variations in formatting and ensures that the data is clean and ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8373ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Normalize text fields before matching ===\n",
    "def normalize_station_name(text):\n",
    "    if isinstance(text, str):\n",
    "        return \" \".join(text.strip().title().split())\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"Service\"] = df[\"Service\"].apply(normalize_station_name)\n",
    "df[\"Departure station\"] = df[\"Departure station\"].apply(normalize_station_name)\n",
    "df[\"Arrival station\"] = df[\"Arrival station\"].apply(normalize_station_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f540554",
   "metadata": {},
   "source": [
    "## Building Reference Lists for Fuzzy Matching\n",
    "In this step, we create reference lists of the most common station names and services. These lists will be used later for fuzzy matching to correct rare or inconsistent entries in the dataset. This ensures data consistency and improves the quality of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2163e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build reference list using top most common station names in your dataset ===\n",
    "top_departures = df[\"Departure station\"].value_counts().head(59).index.tolist()\n",
    "top_arrivals = df[\"Arrival station\"].value_counts().head(59).index.tolist()\n",
    "top_services = df[\"Service\"].value_counts().head(2).index.tolist()\n",
    "\n",
    "print(\"=== TOP DEPARTURES ===\")\n",
    "print(top_departures)\n",
    "\n",
    "print(\"=== TOP ARRIVALS ===\")\n",
    "print(top_arrivals)\n",
    "\n",
    "print(\"=== TOP SERVICES ===\")\n",
    "print(top_services)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4843ce17",
   "metadata": {},
   "source": [
    "## Fuzzy Matching and Applying Fixes\n",
    "To ensure data consistency, we use fuzzy matching to correct rare or inconsistent entries in the dataset. This involves:\n",
    "\n",
    "1. **Fuzzy Matching**: Comparing each entry with a reference list of common values and finding the closest match based on a similarity score.\n",
    "2. **Threshold-Based Correction**: If the similarity score exceeds a predefined threshold, the entry is corrected to the closest match; otherwise, it remains unchanged.\n",
    "3. **Application**: This process is applied to the \"Service,\" \"Departure station,\" and \"Arrival station\" columns using the reference lists created earlier.\n",
    "\n",
    "This step helps in standardizing the data and improving its quality for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cc21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Fuzzy matching function to auto-correct rare entries ===\n",
    "\n",
    "def correct_station_from_top(name, top_list, threshold=75):\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    match, score = process.extractOne(name, top_list)\n",
    "    if score < threshold:\n",
    "        print(\n",
    "            f\"Correcting '{name}' to '{match}' - Score: {score} (Threshold: {threshold})\"\n",
    "        )\n",
    "    # If the score is above the threshold, return the match; otherwise, return the original name\n",
    "    return match if score >= threshold else name\n",
    "\n",
    "\n",
    "# === Apply corrections using internal clean station list ===\n",
    "df[\"Service\"] = df[\"Service\"].apply(lambda x: correct_station_from_top(x, top_services))\n",
    "df[\"Departure station\"] = df[\"Departure station\"].apply(\n",
    "    lambda x: correct_station_from_top(x, top_departures)\n",
    ")\n",
    "df[\"Arrival station\"] = df[\"Arrival station\"].apply(\n",
    "    lambda x: correct_station_from_top(x, top_arrivals)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e84eb62",
   "metadata": {},
   "source": [
    "## Cleaning the Date Column\n",
    "The `Date` column contains some invalid formats that need to be corrected. To clean this column, we:\n",
    "\n",
    "1. **Fix Invalid Formats**: Replace invalid date formats (e.g., `YYYYMM` or `YYYY.MM`) with the correct `YYYY-MM` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167e4d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, clean the Date column by fixing invalid formats\n",
    "df[\"Date\"] = (\n",
    "    # replace invalid formats with NaT (valid formats are YYYY-MM)\n",
    "    df[\"Date\"]\n",
    "    .astype(str)\n",
    "    .apply(\n",
    "        lambda x: re.sub(r\"(\\d{4})([^-])(\\d{2})\", r\"\\1-\\3\", x)\n",
    "        if re.match(r\"\\d{4}[^-]\\d{2}\", x)\n",
    "        else x\n",
    "    )\n",
    ")\n",
    "\n",
    "# Remove date in the future\n",
    "df = df[\n",
    "    df[\"Date\"].apply(lambda x: pd.to_datetime(x, errors=\"coerce\") <= datetime.now())\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c4f16",
   "metadata": {},
   "source": [
    "2. **Convert to Datetime**: Use robust error handling to convert the cleaned strings into proper datetime objects.\n",
    "3. **Handle Missing Values**: Replace invalid or missing dates with `NaT` (Not a Time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc92344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date column to datetime with robust error handling\n",
    "print(\"Converting dates...\")\n",
    "\n",
    "\n",
    "# Use a custom conversion approach with error handling\n",
    "def safe_date_convert(date_str):\n",
    "    if pd.isna(date_str) or date_str == \"\" or date_str == \"N/A\":\n",
    "        return pd.NaT  # Return Not a Time for empty/invalid entries\n",
    "\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format=\"%Y-%m\")\n",
    "    except ValueError:\n",
    "        try:\n",
    "            # Fallback to flexible parsing\n",
    "            return pd.to_datetime(date_str)\n",
    "        except ValueError:\n",
    "            print(f\"Could not parse date: {date_str}\")\n",
    "            return pd.NaT\n",
    "\n",
    "\n",
    "# Apply the conversion\n",
    "df[\"Date\"] = df[\"Date\"].apply(safe_date_convert)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db55889a",
   "metadata": {},
   "source": [
    "4. **Validate date column**: we check the number of valid dates in the dataset and calculate the percentage of valid entries. Rows with invalid or missing dates are filtered out to create a clean dataset for further analysis.\n",
    "\n",
    "This ensures that the `Date` column is consistent and ready for use in time-based analyses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f6ccde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many valid dates we have\n",
    "valid_dates = df[\"Date\"].notna().sum()\n",
    "total_rows = len(df)\n",
    "print(\n",
    "    f\"Successfully converted {valid_dates} out of {total_rows} dates ({valid_dates / total_rows:.2%})\"\n",
    ")\n",
    "\n",
    "# Filter to keep only rows with valid dates if needed\n",
    "df_clean = df[df[\"Date\"].notna()].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38fa5e6",
   "metadata": {},
   "source": [
    "## Creating Additional Date Features\n",
    "To enhance our analysis, we extract additional date-related features from the `Date` column. These features include:\n",
    "\n",
    "1. **Year**: Extracted as a numeric value from the `Date` column.\n",
    "2. **Month**: Extracted as a numeric value from the `Date` column.\n",
    "3. **Season**: Derived from the `Month` column, categorized into:\n",
    "    - **Winter**: December, January, February\n",
    "    - **Spring**: March, April, May\n",
    "    - **Summer**: June, July, August\n",
    "    - **Fall**: September, October, November\n",
    "    - **Unknown**: For missing or invalid months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e45365e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create the additional date features only for valid dates\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "df[\"Month\"] = df[\"Date\"].dt.month\n",
    "df[\"Season\"] = df[\"Date\"].dt.month.apply(\n",
    "    lambda x: \"Winter\"\n",
    "    if pd.notna(x) and x in [12, 1, 2]\n",
    "    else \"Spring\"\n",
    "    if pd.notna(x) and x in [3, 4, 5]\n",
    "    else \"Summer\"\n",
    "    if pd.notna(x) and x in [6, 7, 8]\n",
    "    else \"Fall\"\n",
    "    if pd.notna(x) and x in [9, 10, 11]\n",
    "    else \"Unknown\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1933603",
   "metadata": {},
   "source": [
    "## Understanding the Numerical Data Structure\n",
    "Before diving into the conversion process, we need to establish a clear taxonomy of all numerical columns in our dataset. This approach lets us apply consistent cleaning rules based on each column's characteristics and expected value ranges.\n",
    "\n",
    "### Why This Matters\n",
    "- **Accuracy**: Different column types need different treatment (e.g., percentages vs. durations)\n",
    "- **Consistency**: Applying the same logic to similar columns prevents inconsistencies\n",
    "- **Maintainability**: Centralized definition makes future updates easier\n",
    "- **Documentation**: Serves as self-documenting code that explains column purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337f4822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === NUMERIC COLUMNS REFERENCE LIST ===\n",
    "# This list defines all the numerical features in the dataset that need special handling.\n",
    "# It's used in multiple places:\n",
    "# 1. Converting string values to proper numeric types\n",
    "# 2. Handling outliers and invalid values\n",
    "# 3. Creating correlation matrices\n",
    "# 4. Filling missing values with appropriate strategies (median)\n",
    "#\n",
    "# Each column type has specific cleaning rules applied:\n",
    "# - For all numeric columns: Convert to float, replace negative values with 0\n",
    "# - For average columns: Cap extreme values at 500 minutes (8h30)\n",
    "# - For percentage columns: Ensure values are within 0-100% range\n",
    "\n",
    "numeric_columns = [\n",
    "    # Journey metrics\n",
    "    \"Average journey time\",\n",
    "    # Train count metrics\n",
    "    \"Number of scheduled trains\",\n",
    "    \"Number of cancelled trains\",\n",
    "    \"Number of trains delayed at departure\",\n",
    "    \"Number of trains delayed at arrival\",\n",
    "    \"Number of trains delayed > 15min\",\n",
    "    \"Number of trains delayed > 30min\",\n",
    "    \"Number of trains delayed > 60min\",\n",
    "    # Delay time metrics\n",
    "    \"Average delay of late trains at departure\",\n",
    "    \"Average delay of all trains at departure\",\n",
    "    \"Average delay of late trains at arrival\",\n",
    "    \"Average delay of all trains at arrival\",\n",
    "    \"Average delay of trains > 15min (if competing with flights)\",\n",
    "    # Delay cause percentages\n",
    "    \"Pct delay due to external causes\",\n",
    "    \"Pct delay due to infrastructure\",\n",
    "    \"Pct delay due to traffic management\",\n",
    "    \"Pct delay due to rolling stock\",\n",
    "    \"Pct delay due to station management and equipment reuse\",\n",
    "    \"Pct delay due to passenger handling (crowding, disabled persons, connections)\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6528002d",
   "metadata": {},
   "source": [
    "## Cleaning Each Numeric Column\n",
    "To ensure the quality and consistency of numeric data, we apply the following cleaning steps to each numeric column:\n",
    "\n",
    "1. **Convert to Numeric**: Coerce non-numeric values to `NaN` to handle invalid entries.\n",
    "2. **Handle Negative Values**: Replace negative values with `0` as they are not valid in this context.\n",
    "3. **Cap Extreme Values**:\n",
    "   - For columns representing averages (e.g., delays), cap values at a reasonable maximum of `500` minutes (8 hours and 30 minutes).\n",
    "4. **Validate Percentages**:\n",
    "   - For percentage columns, ensure values are within the range of `0%` to `100%`.\n",
    "   - Cap values exceeding `100%` and floor negative values to `0%`.\n",
    "\n",
    "These steps ensure that the numeric columns are clean, consistent, and ready for analysis or modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb52bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up each numeric column\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        # First convert to numeric, coercing errors to NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "        # Check for negative values and set them to 0\n",
    "        negative = df[col] < 0\n",
    "        if negative.any():\n",
    "            print(f\"Found {negative.sum()} negative values in {col}\")\n",
    "            df.loc[negative, col] = 0.0\n",
    "\n",
    "        # Check for obviously invalid values (like extremely large numbers)\n",
    "        if \"Average\" in col:\n",
    "            # For average columns, cap to reasonable maximum 500 minutes (8h30 max)\n",
    "            max_reasonable_delay = 500.0\n",
    "            outliers = df[col] > max_reasonable_delay\n",
    "            if outliers.any():\n",
    "                print(\n",
    "                    f\"Found {outliers.sum()} outliers in {col} (values > {max_reasonable_delay})\"\n",
    "                )\n",
    "                df.loc[outliers, col] = np.nan  # Set outliers to NaN\n",
    "\n",
    "        # For percentage columns, ensure they are within 0-100 range\n",
    "        if \"Pct\" in col:\n",
    "            # Check for values greater than 100\n",
    "            over_hundred = df[col] > 100\n",
    "            if over_hundred.any():\n",
    "                print(f\"Found {over_hundred.sum()} values over 100% in {col}\")\n",
    "                df.loc[over_hundred, col] = 100.0  # Cap at 100%\n",
    "\n",
    "            # Check for negative values\n",
    "            negative = df[col] < 0\n",
    "            if negative.any():\n",
    "                print(f\"Found {negative.sum()} negative values in {col}\")\n",
    "                df.loc[negative, col] = 0.0  # Floor at 0%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8e2d56",
   "metadata": {},
   "source": [
    "## Handling Missing Values\n",
    "In this section, we address missing values in the dataset to ensure data quality and consistency. The following steps are taken:\n",
    "\n",
    "1. **Identify Missing Values**:\n",
    "    - Columns with missing values are identified, and their percentages are calculated.\n",
    "    - Columns with a high percentage of missing values (e.g., `Cancellation comments` and `Departure delay comments`) are excluded from further analysis.\n",
    "\n",
    "2. **Imputation Strategies**:\n",
    "    - **Numeric Columns**: Missing values are filled with the median of the respective column to minimize the impact of outliers.\n",
    "    - **Categorical Columns**: Missing values are replaced with the most frequent value (mode) in the column.\n",
    "\n",
    "3. **Validation**:\n",
    "    - After imputation, the dataset is checked to ensure no missing values remain in the columns used for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b1855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for columns with very few non-null values\n",
    "sparsity_threshold = (\n",
    "    0.05  # Consider a column sparse if less than 5% of values are non-null\n",
    ")\n",
    "\n",
    "# Check for sparse columns\n",
    "for col in df.columns:\n",
    "    non_null_ratio = df[col].notna().mean()\n",
    "    if non_null_ratio < sparsity_threshold:\n",
    "        print(f\"Column '{col}' has only {non_null_ratio:.2%} non-null values\")\n",
    "\n",
    "# Print summary of numeric columns after cleaning\n",
    "print(\"\\nSummary statistics after cleaning numeric columns:\")\n",
    "print(df[numeric_columns].describe().T[[\"count\", \"mean\", \"min\", \"max\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb299f3c",
   "metadata": {},
   "source": [
    "## Feature Engineering\n",
    "\n",
    "In this section, we create additional features from the existing data to enhance our analysis and predictive modeling capabilities. These engineered features help us extract more value from the raw data and capture important patterns in train delays.\n",
    "\n",
    "### Delay Categorization and Severity Metrics\n",
    "\n",
    "We first categorize delays into meaningful groups based on their severity, which helps simplify analysis and visualization while making the patterns more interpretable:\n",
    "\n",
    "1. **Delay Categories**: \n",
    "   - **Minimal**: Delays under 5 minutes that have negligible impact\n",
    "   - **Moderate**: Delays between 5-15 minutes (noticeable but tolerable)\n",
    "   - **Significant**: Delays between 15-30 minutes (disruptive to travelers)\n",
    "   - **Severe**: Delays over 30 minutes (major disruption to travel plans)\n",
    "\n",
    "2. **Delay Score Aggregation**:\n",
    "   - We calculate a total delay score by combining all percentage contributions from different delay causes\n",
    "   - This provides a single metric that quantifies the overall impact of various delay factors\n",
    "\n",
    "These engineered features transform continuous delay values into more actionable insights that can drive operational improvements and better passenger communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43d167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create delay categories\n",
    "df[\"Delay_Category\"] = pd.cut(\n",
    "    df[\"Average delay of all trains at arrival\"],\n",
    "    bins=[-float(\"inf\"), 5, 15, 30, float(\"inf\")],\n",
    "    labels=[\"Minimal\", \"Moderate\", \"Significant\", \"Severe\"],\n",
    ")\n",
    "\n",
    "# Calculate a delay score (weighted average of different delay percentages)\n",
    "delay_cause_columns = [col for col in df.columns if \"Pct delay due to\" in col]\n",
    "df[\"Total_Delay_Score\"] = df[delay_cause_columns].sum(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35e4b14",
   "metadata": {},
   "source": [
    "## Visualization 1: Delay Distribution\n",
    "Let's visualize the distribution of delays to understand their pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275c20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Delay distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Calculate bin edges at 5-minute intervals\n",
    "bin_edges = np.arange(0, 80 + 5, 5)\n",
    "\n",
    "# Plot histogram with custom bins\n",
    "sns.histplot(df[\"Average delay of all trains at arrival\"].dropna(), bins=50, kde=True)\n",
    "\n",
    "plt.title(\"Distribution of Average Delays at Arrival\")\n",
    "plt.xlabel(\"Delay (minutes)\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xticks(bin_edges)  # Show every other tick to avoid overcrowding\n",
    "\n",
    "# Display in notebook\n",
    "plt.show()\n",
    "\n",
    "# Save to file (optional)\n",
    "plt.savefig(\"delay_distribution.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb994d7c",
   "metadata": {},
   "source": [
    "## Visualization 2: Monthly Trends\n",
    "Let's analyze how delays have evolved over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e26e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Monthly Trends visualization - IMPROVED X-AXIS READABILITY\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_delays = (\n",
    "    df.groupby([\"Year\", \"Month\"])[\"Average delay of all trains at arrival\"]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "monthly_delays[\"YearMonth\"] = (\n",
    "    monthly_delays[\"Year\"]\n",
    "    .astype(int)\n",
    "    .astype(str)  # Convert to int first to remove decimals\n",
    "    + \"-\"\n",
    "    + monthly_delays[\"Month\"]\n",
    "    .astype(int)\n",
    "    .astype(str)\n",
    "    .str.zfill(2)  # Convert to int then to string\n",
    ")\n",
    "plt.plot(\n",
    "    monthly_delays[\"YearMonth\"],\n",
    "    monthly_delays[\"Average delay of all trains at arrival\"],\n",
    "    marker=\"o\",\n",
    ")\n",
    "plt.title(\"Monthly Average Delays\")\n",
    "plt.xlabel(\"Year-Month\")\n",
    "plt.ylabel(\"Average Delay (minutes)\")\n",
    "\n",
    "# Improve x-axis readability by showing only every 4th label\n",
    "x_ticks = plt.xticks()[0]\n",
    "x_labels = plt.xticks()[1]\n",
    "step = 4  # Show every 4th label\n",
    "\n",
    "# Keep only every Nth tick and label\n",
    "plt.xticks(\n",
    "    x_ticks[::step], [label.get_text() for label in x_labels][::step], rotation=45\n",
    ")  # Reduced rotation from 90 to 45 degrees for better readability\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display in notebook\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"monthly_trends.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac79756",
   "metadata": {},
   "source": [
    "## Visualization 3: Stations with Most Delays\n",
    "Let's identify which departure stations experience the most delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21647b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Top departure stations visualization\n",
    "plt.figure(figsize=(12, 8))\n",
    "station_delays = (\n",
    "    df.groupby(\"Departure station\")[\"Average delay of all trains at departure\"]\n",
    "    .mean()\n",
    "    .sort_values(ascending=False)\n",
    "    .head(15)\n",
    ")\n",
    "sns.barplot(x=station_delays.values, y=station_delays.index)\n",
    "plt.title(\"Top 15 Departure Stations with Highest Average Delays\")\n",
    "plt.xlabel(\"Average Delay (minutes)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display in notebook\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"station_delays.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81170f7",
   "metadata": {},
   "source": [
    "## Visualization 4: Causes of Delays\n",
    "Let's understand the main causes behind train delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0556f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Delay causes breakdown\n",
    "plt.figure(figsize=(10, 8))\n",
    "delay_causes = df[delay_cause_columns].mean().sort_values(ascending=False)\n",
    "sns.barplot(\n",
    "    x=delay_causes.values,\n",
    "    y=[col.replace(\"Pct delay due to \", \"\") for col in delay_causes.index],\n",
    ")\n",
    "plt.title(\"Average Percentage of Delays by Cause\")\n",
    "plt.xlabel(\"Average Percentage\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display in notebook\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"delay_causes.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3723b4",
   "metadata": {},
   "source": [
    "## Visualization 5: Correlation Analysis\n",
    "Let's explore the correlations between different numeric variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e530b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Correlation heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "correlation_cols = numeric_columns\n",
    "correlation = df[correlation_cols].corr()\n",
    "mask = np.triu(np.ones_like(correlation, dtype=bool))\n",
    "sns.heatmap(\n",
    "    correlation, mask=mask, annot=True, fmt=\".2f\", cmap=\"coolwarm\", linewidths=0.5\n",
    ")\n",
    "plt.title(\"Correlation Between Numerical Features\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display in notebook\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"correlation_heatmap.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97817d5",
   "metadata": {},
   "source": [
    "## Visualization 6: Seasonal Patterns\n",
    "Let's check if there are seasonal patterns in train delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40896c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Seasonal patterns\n",
    "plt.figure(figsize=(10, 6))\n",
    "seasonal_delays = (\n",
    "    df.groupby(\"Season\")[\"Average delay of all trains at arrival\"]\n",
    "    .mean()\n",
    "    .reindex([\"Winter\", \"Spring\", \"Summer\", \"Fall\"])\n",
    ")\n",
    "sns.barplot(x=seasonal_delays.index, y=seasonal_delays.values)\n",
    "plt.title(\"Average Delays by Season\")\n",
    "plt.ylabel(\"Average Delay (minutes)\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display in notebook\n",
    "plt.show()\n",
    "\n",
    "# Save to file\n",
    "plt.savefig(\"seasonal_patterns.png\")\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7040a7d7",
   "metadata": {},
   "source": [
    "## Final Data Preparation\n",
    "Let's prepare the final dataset for modeling by handling any remaining missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694ae288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data preparation for modeling\n",
    "# Create a more comprehensive feature set\n",
    "print(\"\\nPreparing final dataset for modeling...\")\n",
    "\n",
    "# Fill missing values with appropriate strategies\n",
    "# For numeric columns, fill with median\n",
    "for col in numeric_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "# For categorical columns, fill with most common value\n",
    "categorical_columns = [\"Service\", \"Departure station\", \"Arrival station\", \"Season\"]\n",
    "for col in categorical_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].fillna(df[col].mode()[0])\n",
    "\n",
    "# Create a binary column for significant delays (over 15 minutes)\n",
    "df[\"Is_Significantly_Delayed\"] = (\n",
    "    df[\"Average delay of all trains at arrival\"] > 15\n",
    ").astype(int)\n",
    "\n",
    "# Create a route column\n",
    "df[\"Route\"] = df[\"Departure station\"] + \" to \" + df[\"Arrival station\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d51afa",
   "metadata": {},
   "source": [
    "## Save the Cleaned Dataset\n",
    "Finally, let's save our cleaned dataset and summarize key insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a123c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned and processed dataset\n",
    "df.to_csv(\"cleaned_dataset.csv\", index=False)\n",
    "print(\"\\nCleaned dataset saved to 'cleaned_dataset.csv'\")\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nKey Insights from EDA:\")\n",
    "print(\n",
    "    f\"1. Average delay across all trains: {df['Average delay of all trains at arrival'].mean():.2f} minutes\"\n",
    ")\n",
    "print(\n",
    "    f\"2. Percentage of trains with significant delays (>15min): {(df['Is_Significantly_Delayed'].mean() * 100):.2f}%\"\n",
    ")\n",
    "print(\n",
    "    f\"3. Main cause of delays: {delay_causes.index[0].replace('Pct delay due to ', '')}\"\n",
    ")\n",
    "print(\n",
    "    f\"4. Top delayed route: {df.groupby('Route')['Average delay of all trains at arrival'].mean().sort_values(ascending=False).index[0]}\"\n",
    ")\n",
    "print(f\"5. Season with most delays: {seasonal_delays.idxmax()}\")\n",
    "print(\"\\nEDA complete! The dataset is now ready for predictive modeling.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
